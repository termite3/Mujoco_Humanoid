{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a365a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from callback import SaveOnBestTrainingRewardCallback\n",
    "\n",
    "def train(env_id, log_base_dir=\"logs\", model_base_dir=\"models\", model_name=None, total_timesteps=100000):\n",
    "    \"\"\"\n",
    "    Train a PPO agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_id (str): Environment ID to train on\n",
    "        log_base_dir (str): Base directory for logs\n",
    "        model_base_dir (str): Base directory for saving models\n",
    "        model_name (str): Name for the model file\n",
    "        total_timesteps (int): Total timesteps for training\n",
    "    \"\"\"   \n",
    "    # Create log directory and model directory\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    log_dir = os.path.join(script_dir, log_base_dir, env_id)\n",
    "    model_dir = os.path.join(script_dir, model_base_dir)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Environment\n",
    "    n_envs = 64\n",
    "    env = make_vec_env(env_id, n_envs=n_envs)\n",
    "    env = VecMonitor(env, log_dir)\n",
    "\n",
    "    # Agent Model\n",
    "    if model_name is None:    \n",
    "        model_name = env_id + \"_PPO\"\n",
    "    \n",
    "    policy_kwargs = {\n",
    "        'log_std_init':-2,\n",
    "        'ortho_init': False,\n",
    "    }\n",
    "    \n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        # -------------------------------------------------\n",
    "        learning_rate=1e-3,\n",
    "        n_steps = 500000,\n",
    "        gamma=0.9,\n",
    "        gae_lambda=0.95,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        ent_coef=0.0,\n",
    "        clip_range=0.2,\n",
    "        clip_range_vf=None,\n",
    "        normalize_advantage=True,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        use_sde=True,\n",
    "        sde_sample_freq=4,\n",
    "        rollout_buffer_class=None,\n",
    "        rollout_buffer_kwargs=None,\n",
    "        target_kl=None,\n",
    "        stats_window_size=100,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        # -------------------------------------------------\n",
    "        tensorboard_log = log_dir,\n",
    "        verbose=1,\n",
    "        seed=None,\n",
    "        device='auto',\n",
    "    )            \n",
    "\n",
    "\n",
    "    # Train\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)   \n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=callback,\n",
    "        log_interval=4,\n",
    "        tb_log_name=\"PPO\",\n",
    "        reset_num_timesteps=True,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    save_path = os.path.join(model_dir, model_name)\n",
    "    model.save(save_path)\n",
    "\n",
    "    # close the environment\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def run(env_id, model_base_dir=\"models\", model_name=None, n_episodes=5):\n",
    "    \"\"\"\n",
    "    Run a trained PPO agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_id (str): Environment ID to run on\n",
    "        model_base_dir (str): Base directory for loading models\n",
    "        model_name (str): Name of the model file\n",
    "        n_episodes (int): Number of episodes to run\n",
    "    \"\"\"\n",
    "    # Environment\n",
    "    env = gym.make(env_id,\n",
    "                   xml_file='humanoid.xml',\n",
    "                   forward_reward_weight=1.0,\n",
    "                   ctrl_cost_weight=0.1,\n",
    "                   contact_cost_weight=5e-7,\n",
    "                   healthy_reward=5.0,\n",
    "                   terminate_when_unhealthy=True,\n",
    "                   healthy_z_range=(1.0, 2.0),\n",
    "                   reset_noise_scale=1e-2,\n",
    "                   exclude_current_positions_from_observation=True,\n",
    "                   )\n",
    "    \n",
    "    # Model\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    if model_name is None:\n",
    "        model_name = env_id + \"_PPO\"\n",
    "    model_path = os.path.join(script_dir, model_base_dir, model_name)\n",
    "    model = PPO.load(model_path, env)\n",
    "\n",
    "    # Run    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        episode_reward = 0\n",
    "        n_standing_steps = 0\n",
    "        while True:\n",
    "            action, state = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            time.sleep(0.01)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Count steps where pendulum is standing (reward > threshold)\n",
    "            if reward > -1.0:\n",
    "                n_standing_steps += 1\n",
    "                \n",
    "            if terminated or truncated:\n",
    "                time.sleep(1.0)\n",
    "                print(f\"Episode {episode + 1}: Total reward = {episode_reward:.2f}, Standing steps = {n_standing_steps}/200\")\n",
    "                episode_reward = 0\n",
    "                n_standing_steps = 0\n",
    "                break\n",
    "            \n",
    "    # close the environment\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     env_id = \"Humanoid-v5\"\n",
    "    \n",
    "#     train(env_id)\n",
    "#     # run(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd90106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/kimm-linux/Documents/Cource/2025_PHD/AdvancedRL/termPro/.venv/lib/python3.11/site-packages/stable_baselines3/\n",
       "common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you \n",
       "are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/kimm-linux/Documents/Cource/2025_PHD/AdvancedRL/termPro/.venv/lib/python3.11/site-packages/stable_baselines3/\n",
       "common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you \n",
       "are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using cuda device\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using cuda device\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/kimm-linux/Documents/Cource/2025_PHD/AdvancedRL/termPro/.venv/lib/python3.11/site-packages/stable_baselines3/\n",
       "common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended \n",
       "to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See \n",
       "https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export \n",
       "CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and \n",
       "the training might take longer than on CPU.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/kimm-linux/Documents/Cource/2025_PHD/AdvancedRL/termPro/.venv/lib/python3.11/site-packages/stable_baselines3/\n",
       "common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended \n",
       "to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See \n",
       "https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export \n",
       "CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and \n",
       "the training might take longer than on CPU.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "log_base_dir=\"logs\"\n",
    "env_id = \"Humanoid-v5\"\n",
    "model_base_dir=\"models\"\n",
    "model_name=None\n",
    "total_timesteps=100000\n",
    "\n",
    "# script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "script_dir = os.getcwd()\n",
    "log_dir = os.path.join(script_dir, log_base_dir, env_id)\n",
    "model_dir = os.path.join(script_dir, model_base_dir)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Environment\n",
    "n_envs = 64\n",
    "env = make_vec_env(env_id, n_envs=n_envs)\n",
    "env = VecMonitor(env, log_dir)\n",
    "\n",
    "# Agent Model\n",
    "if model_name is None:    \n",
    "    model_name = env_id + \"_PPO\"\n",
    "\n",
    "policy_kwargs = {\n",
    "    'log_std_init':-2,\n",
    "    'ortho_init': False,\n",
    "}\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    # -------------------------------------------------\n",
    "    learning_rate=1e-3,\n",
    "    n_steps = 500000,\n",
    "    gamma=0.9,\n",
    "    gae_lambda=0.95,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2,\n",
    "    clip_range_vf=None,\n",
    "    normalize_advantage=True,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    use_sde=True,\n",
    "    sde_sample_freq=4,\n",
    "    rollout_buffer_class=None,\n",
    "    rollout_buffer_kwargs=None,\n",
    "    target_kl=None,\n",
    "    stats_window_size=100,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    # -------------------------------------------------\n",
    "    tensorboard_log = log_dir,\n",
    "    verbose=1,\n",
    "    seed=None,\n",
    "    device='auto',\n",
    ")            \n",
    "\n",
    "\n",
    "# Train\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)   \n",
    "# model.learn(\n",
    "#     total_timesteps=total_timesteps,\n",
    "#     callback=callback,\n",
    "#     log_interval=4,\n",
    "#     tb_log_name=\"PPO\",\n",
    "#     reset_num_timesteps=True,\n",
    "#     progress_bar=True,\n",
    "# )\n",
    "\n",
    "# Save the trained model\n",
    "save_path = os.path.join(model_dir, model_name)\n",
    "model.save(save_path)\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
